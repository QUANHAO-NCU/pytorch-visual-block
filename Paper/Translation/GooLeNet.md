# 更深层次的卷积

### 摘要

我们提出了一种称为**Inception**(开端;盗梦空间)的深度卷积神经网络架构，该架构在**ImageNet的2014年大规模视觉识别挑战赛**(ImageNet Large-Scale Visual Recognition Challenge 2014，ILSVRC 14)上达到**最佳表现**(state of the art,SOTA)。该架构的主要特点是提升了网络内**计算资源**(**解读1**)的利用率。这个架构经过精心设计，提升了网络的深度和宽度，同时保持计算预算不变。为了提升网络质量，网络的架构是基于Hebbian原则和多尺度处理的直觉。我们提交到ILSVRC14比赛中的一个网络版本是GoogLeNet，这是一个22层深的网络，它的性能是在分类和检测任务下进行评估的。

### 介绍

在过去的三年里，由于深度学习的进步，更具体地说是卷积网络，图像识别和目标检测的质量一直在以惊人的速度发展。一个令人鼓舞的消息是，这一进步不仅仅是更强大的硬件、更大的数据集和更大的模型的结果，更是新思想、新算法和改进的网络架构的结果。例如，除了用于检测目的的同一个竞赛的分类数据集之外，ILSVRC 2014竞赛的顶级参赛作品没有使用新的数据源。我们向ILSVRC 2014提交的GoogLeNet实际上使用了比两年前Krizhevsky等人的获奖架构（AlexNet）少12倍的参数，同时更加准确。目标检测的最大收益并不仅仅来自于深度网络或更大模型的利用，而是来自于深度架构和经典计算机视觉的协同作用，如Girshick等人的R-CNN算法。

另一个值得注意的因素是，随着移动和嵌入式计算的持续发展，我们算法的效率——尤其是它们的功耗和内存使用——变得越来越重要。值得注意的是，导致本文提出的深层结构设计的考虑因素包括了这一因素，而不是单纯地专注于精确数字。在大多数实验中，这些模型被设计成在推理时保持15亿次乘法加法的计算预算，这样它们就不会成为纯粹的学术好奇心，而是可以以合理的成本在现实世界中使用，即使是在大型数据集上。

在本文中，我们将重点研究一种高效的计算机视觉深层神经网络体系结构，代号为盗梦空间(Inception)，其名称来源于林等人[12]的网络论文中的网络与著名的“我们需要更深入”的互联网模因[1]。在我们的案例中，“深度”一词有两种不同的含义:首先，在某种意义上，我们以“inception module”的形式引入了一个新的组织层次，也在更直接的意义上增加了网络深度。总的来说，人们在尝试从Arora等人的理论工作中获得灵感和指导时可以将inception module视为逻辑顶点。同时，在ILSVRC 2014分类和检测挑战中，该架构的优势得到了实验验证，在这方面，它明显优于当前的技术水平。

### 相关工作

从LeNet-5 开始，卷积神经网络(CNN)通常具有标准结构——堆叠卷积层(可选地后跟不同的归一化和最大池化)后跟一个或多个全连接层。这种基本设计的五种变体在图像分类文献中很流行，并且在MNIST、CIFA和最著名的ImageNet分类挑战中取得了迄今为止最好的结果。对于更大的数据集，如Imagenet，最近的趋势是增加图层数量和图层大小，同时使用Dropout来解决过度拟合的问题。

尽管担心最大池层会导致精确空间信息的丢失，但与(AlexNet)相同的卷积网络架构也已成功用于定位，目标检测和人体姿态估计。受灵长类视觉皮层神经科学模型的启发，Serre等人[15]使用了一系列固定的生物过滤器来处理多种尺度，类似于初始模型。然而，与固定的2层深度模型相反，Inception module中的所有过滤器都是学习的。此外，Inception层被重复多次，造就了22层深度的GoogLeNet模型。

网络到网络(Network in Network)是林等人为了增加神经网络的表示能力而提出的方法。当应用于卷积层时，该方法可被视为附加的1×1卷积层，之后通常是校正的线性激活[9]。这使得它能够很容易地集成到当前的CNN管道中。我们在架构中大量使用这种方法。然而，在我们的环境中，1 × 1卷积有双重目的:最关键的是，它们主要用作降维模块，以消除计算瓶颈，否则会限制我们网络的大小。这不仅可以增加网络的深度，还可以增加网络的宽度，而不会显著影响性能。

