# 用于图像识别的深度残差学习

## 摘要

​	更深的网络更难以训练。我们提出了一个残差学习框架以简化一个比以往网络更深的网络的训练。我们将每一层的任务重构为对于输入的残差函数的学习，而不是直接学习未知的输入的特征函数。我们提供了全面的经验证据标明，学习残差函数更容易优化，而且在网络深度增加时依然能得到精度提升。在ImageNet数据集上，我们评估了最深达152层的残差网络——这比VGG网络深8倍，却拥有更低的复性。这一系列的残差网络在ImageNet数据集上取得了3.75%的误差。该结果在ILSVRC 2015分类任务中获得第一名。我们还在CIFA10数据集上用100层和1000层的网络进行了分析。

​	表征的深度对于许多视觉识别任务至关重要。我们仅使用更深的表征，就在COCO目标检测数据集上获得了28%的改进。我们还在ImageNet图像检测，ImageNet本地化，COCO检测，COCO语义分割任务上获得了第一名。

## 介绍

​	深度卷积网络推动了图像识别领域的一系列突破。深度网络在多层网络中自然地集成了低/中/高层特征和一个“端到端”的分类器，随着网络深度的增加，特征的“层次”会不断丰富。最近的证据标明，网络的深度至关重要，以至于在充满挑战的ImageNet数据集上都使用了16层或19层等“非常深”的模型。非常深的模型在其他特别的视觉识别任务同样受益。

​	在深度的驱动下，有一个问题：学习一个更好网络是否会像堆叠一个更深的网络一样容易？回答这个问题有一个障碍，那就是臭名昭著的梯度消失/梯度爆炸问题，这从一开始就阻碍了网络能否收敛。然而标准化初始化和中间层标准化（归一化）在一定程度上解决了这个问题，这使得几十层的网络可以在具有反向传播的随机梯度下降（SGD）方法下收敛。

​	当更深的网络开始收敛时，一个退化的问题就暴露出来：随着网络深度的增加，精度达到饱和（情理之中），然后迅速退化，令人意外的是，这种退化不是由于过拟合引起的，给深度模型中添加更多层会导致更高的训练误差。正如图 1展示的那样。

<img src="RestNet_images\image-20210524110436649.png" alt="image-20210524110436649" style="zoom:150%;" />

`图 1 在CIFA-10数据集上使用20层和56层“平坦”网络的训练误差（左）和测试误差（右）。更深的网络有更高的训练误差和测试误差。在ImageNet数据集上有相似的现象，展示在图4`

​	（训练精度的）下降表明，并不是所有的系统都同样容易优化。让我们考察这样一个网络，它有浅层版本，通过堆叠更多网络层可以构建它的深层版本。有这样一种构建它的深层版本的方法：添加层是一种特征函数，其他层是已学习的浅层的副本。这种构造方案的结果表明，更深的模型不会比较浅的模型造成更多的训练误差。实验表明，我们现有的解决方案无法找到同等或者更好的方案（在可行的时间内）。

​	在本文中，我们引入深度残差学习框架来解决退化问题。相比于期望让几个堆叠的网络层直接学习到一个根本的函数，我们直接让这些网络层去学习一个残差函数。形式上，把期望的根本函数定义为$H(x)$ ,我们堆叠的其他块则是要适配到另一个函数：$F(x) = H(x) - x$ 。那么原始函数就可以表示为$F(x)+x$ 。我们假设优化残差函数比优化原先的，未经验证的根本函数要容易。在极端条件下，如果添加的特征函数是最优的，那么将残差函数推至零比通过一堆非线性层来拟合特征函数要容易。

公式 $F(x)+x$ 可以视为带有“捷径”的前馈神经网络（图2）。所谓快捷连接是指跳过一层或多层。我们的示例中，快捷连接用于简单的表示特征映射。它们的输出会被加到堆叠层的输出上。特征映射的快捷连接既不会增加额外的参数，也不会增加计算复杂性。整个网络仍然可以由SGD通过反向传播进行端到端的训练，并且可以使用公共库（如Caffe）轻松实现，无须修改解算器。

我们在ImageNet数据集上进行全面实验，以显示退化问题并评估我们的方法。



