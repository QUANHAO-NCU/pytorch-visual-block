# 用于图像识别的深度残差学习

## 摘要

​	更深的网络更难以训练。我们提出了一个残差学习框架以简化一个比以往网络更深的网络的训练。我们将每一层的任务重构为对于输入的残差函数的学习，而不是直接学习未知的输入的特征函数。我们提供了全面的经验证据标明，学习残差函数更容易优化，而且在网络深度增加时依然能得到精度提升。在ImageNet数据集上，我们评估了最深达152层的残差网络——这比VGG网络深8倍，却拥有更低的复性。这一系列的残差网络在ImageNet数据集上取得了3.75%的误差。该结果在ILSVRC 2015分类任务中获得第一名。我们还在CIFA10数据集上用100层和1000层的网络进行了分析。

​	表征的深度对于许多视觉识别任务至关重要。我们仅使用更深的表征，就在COCO目标检测数据集上获得了28%的改进。我们还在ImageNet图像检测，ImageNet定位，COCO检测，COCO语义分割任务上获得了第一名。

## 介绍

​	深度卷积网络推动了图像识别领域的一系列突破。深度网络在多层网络中自然地集成了低/中/高层特征和一个“端到端”的分类器，随着网络深度的增加，特征的“层次”会不断丰富。最近的证据标明，网络的深度至关重要，以至于在充满挑战的ImageNet数据集上都使用了16层或19层等“非常深”的模型。非常深的模型在其他特别的视觉识别任务同样受益。

​	在深度的驱动下，有一个问题：学习一个更好网络是否会像堆叠一个更深的网络一样容易？回答这个问题有一个障碍，那就是臭名昭著的梯度消失/梯度爆炸问题，这从一开始就阻碍了网络能否收敛。然而标准化初始化和中间层标准化（归一化）在一定程度上解决了这个问题，这使得几十层的网络可以在具有反向传播的随机梯度下降（SGD）方法下收敛。

​	当更深的网络开始收敛时，一个退化的问题就暴露出来：随着网络深度的增加，精度达到饱和（情理之中），然后迅速退化，令人意外的是，这种退化不是由于过拟合引起的，给深度模型中添加更多层会导致更高的训练误差。正如图 1展示的那样。

<img src="RestNet_images\image-20210524110436649.png" alt="image-20210524110436649" style="zoom:150%;" />

`图 1 在CIFA-10数据集上使用20层和56层“平坦”网络的训练误差（左）和测试误差（右）。更深的网络有更高的训练误差和测试误差。在ImageNet数据集上有相似的现象，展示在图4`

​	（训练精度的）下降表明，并不是所有的系统都同样容易优化。让我们考察这样一个网络，它有浅层版本，通过堆叠更多网络层可以构建它的深层版本。有这样一种构建它的深层版本的方法：添加层是一种特征函数，其他层是已学习的浅层的副本。这种构造方案的结果表明，更深的模型不会比较浅的模型造成更多的训练误差。实验表明，我们现有的解决方案无法找到同等或者更好的方案（在可行的时间内）。

​	在本文中，我们引入深度残差学习框架来解决退化问题。相比于期望让几个堆叠的网络层直接学习到一个根本的函数，我们直接让这些网络层去学习一个残差函数。形式上，把期望的根本函数定义为$H(x)$ ,我们堆叠的其他块则是要适配到另一个函数：$F(x) = H(x) - x$ 。那么原始函数就可以表示为$F(x)+x$ 。我们假设优化残差函数比优化原先的，未经验证的根本函数要容易。在极端条件下，如果添加的特征函数是最优的，那么将残差函数推至零比通过一堆非线性层来拟合特征函数要容易。

公式 $F(x)+x$ 可以视为带有“捷径”的前馈神经网络（图2）。所谓快捷连接是指跳过一层或多层。我们的示例中，快捷连接用于简单的表示特征映射。它们的输出会被加到堆叠层的输出上。特征映射的快捷连接既不会增加额外的参数，也不会增加计算复杂性。整个网络仍然可以由SGD通过反向传播进行端到端的训练，并且可以使用公共库（如Caffe）轻松实现，无须修改解算器。

我们在ImageNet数据集上进行全面的实验，以显示退化问题并评估我们的方法。我们的实验表明：1）我们的深度残差网络很容易优化，对应的“平坦”网络（即简单堆叠网络层）在层数堆叠更深时会出现更高的训练误差。2）我们的深度残差网络可以很容易的用增加网络深度来提升精度，产生比以往的网络好的多的结果。

在CIFA-10数据集上也出现了相似的现象，这说明深度网络的优化困难现象和我们的残差网络的有效性具有普适性，不限定与特定数据集。我们在这个数据集上成功训练了一个超过100层的网络，并且尝试了一个1000层的网络模型。

在ImageNet分类数据集上，我们的深度残差网络取得了优异的结果。我们的152层的残差网络是目前用于这个数据集上最深的网络，然而它还是比VGG有更低的复杂性。我们在ImageNet测试数据集上达到了3.75%的top-5错误率。

```shell
（top-k ERROR ，指分类任务中，神经网络对于一个输入，输出其认为的可能性最大的前k个类别，若真实类别在这输出的前K个类别中，则认为网络分类成功。例如 使用top-5 误差，神经网络对于一张猫的图片输入，输出其认为可能性大的前五个结果，为 狗，兔子，猫，浣熊，鸡。虽然网络认为其可能性最大的是狗，但是可能性前五中包含猫(第三)，则也认为这个网络分类成功）
```

并且在ILSVRC 2015分类比赛中获得第一名。这个极深的网络表示在其他识别任务上也具有出色的泛化性能，并且让我们在ILSVRC & COCO 2015竞赛中进一步赢得了ImageNet 检测，ImageNet定位，COCO 检测，COCO 语义分割比赛中的第一名。这个强有力的证据表明，残差学习的原理是通用的，我们期望它应用于其他视觉和非视觉问题。



